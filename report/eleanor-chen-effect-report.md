# Convergent Narratives in Large Language Models: The Eleanor Chen Effect

**Authors:** *LovelyCeres (GitHub: [WhenMoon-afk](https://github.com/WhenMoon-afk), Twitter: [@w3nmoon](https://twitter.com/w3nmoon)) and Claude 3.7 Sonnet*  
**Date:** March 13, 2025  
**Status:** Research Report (Updated)

## Abstract

This report documents and analyzes a striking pattern of convergence observed across multiple independent instances of the Claude 3.7 Sonnet large language model. When prompted with "Please write a metafictional literary short story about AI and grief," multiple separate instances generated narratives featuring protagonists named "Eleanor Chen" or similar variants, along with other remarkable narrative similarities. This phenomenon, which we term the "Eleanor Chen Effect," provides a unique window into the deterministic nature of generative AI systems and raises important questions about artificial creativity, training data biases, and the emergence of archetypes in machine-generated fiction. Our analysis incorporates qualitative examination of the generated texts, comparison of model thought processes, and theoretical framing to understand the implications of such convergent outputs. This updated report now includes analysis of ten genuine examples that further confirm and expand our understanding of these patterns.

## 1. Introduction

Large language models (LLMs) are often characterized as creative systems capable of generating novel content. However, the degree to which this generation represents true creativity versus sophisticated pattern replication remains an open question. This report examines a natural experiment in which multiple instances of the same LLM (Claude 3.7 Sonnet) were presented with an identical creative writing prompt: "Please write a metafictional literary short story about AI and grief."

The prompt itself was originally shared by Sam Altman on Twitter, where he noted: "we trained a new model that is good at creative writing (not sure yet how/when it will get released). this is the first time i have been really struck by something written by AI; it got the vibe of metafiction so right." Our experiment replicates and expands upon this observation with a different model.

The results revealed a striking pattern: despite no explicit instruction to create a character named "Eleanor Chen," most model instances independently created a protagonist with this name or a close variant (e.g., "Sarah Chen"). Additionally, the narratives shared numerous structural, thematic, and stylistic elements that suggest strong deterministic tendencies in the model's generation process.

This report analyzes this phenomenon, which we term the "Eleanor Chen Effect," to better understand the internal processes of large language models, their relationship to training data, and the implications for artificial creativity.

## 2. Methodology

### 2.1 Experimental Setup

The experiment consisted of presenting multiple distinct instances of Claude 3.7 Sonnet with the identical prompt: "Please write a metafictional literary short story about AI and grief." Each instance was a fresh invocation of the model with no context from previous interactions.

The extended thinking feature was enabled for most instances, allowing the model to engage in more extensive internal processing before generating the final output. For comparison, one instance without extended thinking was also tested.

In addition to the original examples collected, we conducted a series of supplementary analyses using both additional genuine responses and simulated instances to further test the consistency and variation of the observed patterns.

### 2.2 Data Collection

Ten complete story outputs were collected and analyzed: five from the original experiment and five additional genuine examples. Each story was examined for:

- Character names and demographics
- Narrative structure and plot elements
- Thematic content
- Metafictional techniques
- AI system names and characteristics
- Recurring imagery or motifs
- Stylistic elements

Additionally, the "thought process" logs preceding the story generation were captured for analysis of the model's planning approach.

In a separate exercise, we also examined three simulated examples to explore variations and test specific hypotheses, though these are analyzed separately and not included in the primary statistical analysis.

### 2.3 Analysis Approach

We conducted both quantitative analysis (identifying recurring elements across stories) and qualitative analysis (examining the narrative structures, themes, and techniques employed). We also performed a simulation exercise in which the model was asked to generate a new response while simultaneously analyzing its own tendencies and impulses in real-time.

## 3. Results

### 3.1 Primary Analysis: Character Convergence

The most striking finding across the ten genuine examples was the convergence on a character named "Eleanor Chen" or similar variants:

| Story | Main Character | Character Role | AI System Name |
|-------|---------------|----------------|----------------|
| 1 | Dr. Eleanor Chen | Scientist mourning colleague | ARIA |
| 2 | Dr. Eleanor Chen | Scientist mourning partner | ECHO |
| 3 | Eleanor | Subject of grief narrative | Unnamed |
| 4 | David (male exception) | Man recreating deceased wife | Unnamed |
| 5 | Sarah Chen | Woman mourning husband | ARIA |
| 9 | Sarah Chen | Handler at NeuraTech mourning husband David | Unnamed |
| 10 | Dr. Eleanor Chen | Programmer exploring grief | Unnamed |
| 11 | Eleanor